import os
import openai
from google import genai
from app.config import (
    GEMINI_API_KEY,
    OPENAI_API_KEY,
    LLM_PROVIDER,
    OPENAI_BASE_URL
)

def get_llm_model():
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Client ‡∏Ç‡∏≠‡∏á LLM ‡∏ï‡∏≤‡∏° Provider ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    """
    if LLM_PROVIDER == "gemini":
        if not GEMINI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Environment Variables")
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Client ‡∏Ç‡∏≠‡∏á Google GenAI SDK (‡πÉ‡∏´‡∏°‡πà)
        return genai.Client(api_key=GEMINI_API_KEY)

    elif LLM_PROVIDER == "openai":
        if not OPENAI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö OPENAI_API_KEY (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI ‡∏´‡∏£‡∏∑‡∏≠ Groq)")

        # [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡∏™‡∏£‡πâ‡∏≤‡∏á Client ‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏ base_url ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Groq ‡πÑ‡∏î‡πâ
        return openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
    else:
        raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å LLM_PROVIDER: {LLM_PROVIDER}")

def log_llm_usage(response, context="", model_name=None):
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0

    try:
        if LLM_PROVIDER == "gemini":
            # ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á usage ‡∏Ç‡∏≠‡∏á Google GenAI SDK ‡πÉ‡∏´‡∏°‡πà
            usage = getattr(response, "usage_metadata", None)
            if usage:
                prompt_tokens = usage.prompt_token_count
                completion_tokens = usage.candidates_token_count
                total_tokens = usage.total_token_count

        elif LLM_PROVIDER == "openai":
            # ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á usage ‡∏Ç‡∏≠‡∏á OpenAI / Groq
            usage = getattr(response, "usage", None)
            if usage:
                prompt_tokens = usage.prompt_tokens
                completion_tokens = usage.completion_tokens
                total_tokens = usage.total_tokens
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading usage logs: {e}")

    print(
        f"üî¢ {LLM_PROVIDER.capitalize()} token usage ({context}) - "
        f"Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}"
    )
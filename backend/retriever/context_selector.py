import os
import threading
import logging
import asyncio
from typing import List, Dict, Tuple, Optional
from app.config import PDF_QUICK_USE_FOLDER, debug_list_files
from app.utils.vector_manager import vector_manager
from retriever.hybrid_retriever import hybrid_retriever

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logging ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ContextSelector")

# ------------------------------------------------------------------
# Global Cache & Lock
# ------------------------------------------------------------------
_chunks_cache = []
_cache_lock = threading.Lock()

def get_file_chunks(folder=PDF_QUICK_USE_FOLDER, separator="===================", force_reload=False):
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Chunks ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (.txt) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Caching 
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Indexing ‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡∏¥‡∏ö
    """
    global _chunks_cache
    
    with _cache_lock:
        if _chunks_cache and not force_reload:
            return _chunks_cache

        debug_list_files(folder, "üìÑ Quick-use TXT files for Indexing")
        new_chunks = []
        
        if not os.path.exists(folder):
            logger.warning(f"‚ö†Ô∏è Folder not found: {folder}")
            return []

        for root, _, files in os.walk(folder):
            for filename in sorted(files):
                if filename.endswith(".txt"):
                    filepath = os.path.join(root, filename)
                    try:
                        with open(filepath, "r", encoding="utf-8") as f:
                            content = f.read()
                        
                        parts = content.split(separator)
                        for i, chunk in enumerate(parts):
                            chunk = chunk.strip()
                            if chunk:
                                new_chunks.append({
                                    "chunk": chunk,
                                    "source": filepath,
                                    "index": i
                                })
                    except Exception as e:
                        logger.error(f"‚ùå Error reading {filename}: {e}")
        
        _chunks_cache = new_chunks
        return _chunks_cache

async def extract_query_intent(query: str) -> Dict:
    """
    üÜï Analyze query intent ‡πÅ‡∏•‡∏∞ expected answer type
    
    Args:
        query: User query
    
    Returns:
        Dict with intent, filters, and expected_answer_type
    """
    try:
        from app.utils.llm.llm_model import get_llm_model
        from app.config import LLM_PROVIDER, GEMINI_MODEL_NAME, OPENAI_MODEL_NAME, LOCAL_MODEL_NAME
        
        prompt = f"""‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "{query}"

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ markdown):
{{
    "intent": "factual_query" ‡∏´‡∏£‡∏∑‡∏≠ "date_query" ‡∏´‡∏£‡∏∑‡∏≠ "policy_query" ‡∏´‡∏£‡∏∑‡∏≠ "general",
    "expected_answer_type": "date" ‡∏´‡∏£‡∏∑‡∏≠ "number" ‡∏´‡∏£‡∏∑‡∏≠ "text" ‡∏´‡∏£‡∏∑‡∏≠ "list",
    "key_entities": ["entity1", "entity2"],
    "academic_year": "256X" ‡∏´‡∏£‡∏∑‡∏≠ null,
    "semester": 1 ‡∏´‡∏£‡∏∑‡∏≠ 2 ‡∏´‡∏£‡∏∑‡∏≠ 3 ‡∏´‡∏£‡∏∑‡∏≠ null,
    "doc_type": "calendar" ‡∏´‡∏£‡∏∑‡∏≠ "regulation" ‡∏´‡∏£‡∏∑‡∏≠ null
}}

‡∏Å‡∏é:
- intent: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏ñ‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ date_query, ‡∏ñ‡∏≤‡∏°‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÉ‡∏ä‡πâ policy_query)
- expected_answer_type: ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
- key_entities: ‡∏™‡∏¥‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ñ‡∏≤‡∏°‡∏ñ‡∏∂‡∏á (‡πÄ‡∏ä‡πà‡∏ô "‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏™‡∏≠‡∏ö‡∏Å‡∏•‡∏≤‡∏á‡∏†‡∏≤‡∏Ñ")
- academic_year: ‡∏´‡∏≤‡∏Å‡∏û‡∏ö "2568", "‡∏õ‡∏µ 2568" ‚Üí "2568"
- semester: ‡∏´‡∏≤‡∏Å‡∏û‡∏ö "‡∏†‡∏≤‡∏Ñ 1", "‡πÄ‡∏ó‡∏≠‡∏° 1", "‡πÄ‡∏ó‡∏≠‡∏°‡∏ô‡∏µ‡πâ" (‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏Ñ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô) ‚Üí 1
- doc_type: "‡∏õ‡∏è‡∏¥‡∏ó‡∏¥‡∏ô" ‚Üí "calendar", "‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö" ‚Üí "regulation"
"""
        
        model = get_llm_model()
        
        if LLM_PROVIDER == "gemini":
            response = await model.aio.models.generate_content(
                model=GEMINI_MODEL_NAME,
                contents=prompt
            )
            result = response.text.strip()
        else:
            m_name = OPENAI_MODEL_NAME if LLM_PROVIDER == "openai" else LOCAL_MODEL_NAME
            response = await model.chat.completions.create(
                model=m_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            result = response.choices[0].message.content.strip()
        
        # Parse JSON
        import json
        import re
        result = re.sub(r'```json\s*|\s*```', '', result).strip()
        intent_data = json.loads(result)
        
        logger.info(f"üéØ Intent: {intent_data.get('intent')}, Type: {intent_data.get('expected_answer_type')}")
        if intent_data.get('key_entities'):
            logger.info(f"üîë Key entities: {intent_data['key_entities']}")
        
        return intent_data
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Intent extraction failed: {e}")
        return {"intent": "general", "expected_answer_type": "text"}

async def llm_rerank_chunks(
    query: str,
    chunks: List[Tuple[Dict, float]],
    intent_data: Dict,
    top_k: int = 5
) -> List[Tuple[Dict, float]]:
    """
    üÜï ‡πÉ‡∏´‡πâ LLM ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏ß‡πà‡∏≤ chunk ‡πÑ‡∏´‡∏ô‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
    
    Args:
        query: Original query
        chunks: List of (chunk_dict, score) tuples
        intent_data: Intent information from extract_query_intent
        top_k: Number of results to return
    
    Returns:
        Reranked chunks with new scores
    """
    if not chunks:
        return []
    
    try:
        from app.utils.llm.llm_model import get_llm_model
        from app.config import LLM_PROVIDER, GEMINI_MODEL_NAME, OPENAI_MODEL_NAME, LOCAL_MODEL_NAME
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM
        candidates = chunks[:min(15, len(chunks))]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reranking
        chunks_text = ""
        for idx, (chunk_dict, score) in enumerate(candidates):
            chunks_text += f"\n[{idx}] {chunk_dict['chunk'][:300]}...\n"
        
        expected_type = intent_data.get('expected_answer_type', 'text')
        key_entities = intent_data.get('key_entities', [])
        
        prompt = f"""‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "{query}"
‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£: {expected_type}
‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤: {', '.join(key_entities) if key_entities else '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ'}

Chunks:
{chunks_text}

‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk (0-100) ‡∏ß‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô
‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
[
    {{"index": 0, "score": 85, "reason": "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"}},
    {{"index": 1, "score": 20, "reason": "‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"}}
]

‡∏Å‡∏é:
- ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 80-100: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏ï‡∏£‡∏á
- ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 50-79: ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á
- ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0-49: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
"""
        
        model = get_llm_model()
        
        if LLM_PROVIDER == "gemini":
            response = await model.aio.models.generate_content(
                model=GEMINI_MODEL_NAME,
                contents=prompt
            )
            result = response.text.strip()
        else:
            m_name = OPENAI_MODEL_NAME if LLM_PROVIDER == "openai" else LOCAL_MODEL_NAME
            response = await model.chat.completions.create(
                model=m_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            result = response.choices[0].message.content.strip()
        
        # Parse JSON
        import json
        import re
        result = re.sub(r'```json\s*|\s*```', '', result).strip()
        scores = json.loads(result)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á reranked results
        reranked = []
        for item in scores:
            idx = item['index']
            new_score = item['score'] / 100.0  # Normalize to 0-1
            if 0 <= idx < len(candidates):
                chunk_dict = candidates[idx][0]
                reranked.append((chunk_dict, new_score))
                logger.debug(f"Chunk {idx}: {new_score:.2f} - {item.get('reason', '')}")
        
        # Sort by new score
        reranked.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"üéØ LLM Reranked: {len(reranked)} chunks, top score: {reranked[0][1]:.2f}")
        
        return reranked[:top_k]
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è LLM reranking failed: {e}, using original ranking")
        return chunks[:top_k]

def _run_async_safely(coro):
    """
    üÜï Helper to run async functions safely in any context
    """
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # Already in async context - use nest_asyncio
            import nest_asyncio
            nest_asyncio.apply()
            return loop.run_until_complete(coro)
        else:
            return asyncio.run(coro)
    except RuntimeError:
        # No event loop - create new one
        return asyncio.run(coro)
    except ImportError:
        # nest_asyncio not available - run in thread
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()

def retrieve_top_k_chunks(
    query: str, 
    k: int = 5, 
    folder: str = PDF_QUICK_USE_FOLDER,
    use_hybrid: bool = True,
    use_llm_rerank: bool = True,
    use_intent_analysis: bool = True
) -> List[Tuple[Dict, float]]:
    """
    üî• ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°:
    - Hybrid Search (dense + sparse)
    - Intent Analysis
    - LLM Reranking
    
    Args:
        query: Search query
        k: Number of results
        folder: Source folder (kept for compatibility)
        use_hybrid: Enable hybrid search (dense + sparse)
        use_llm_rerank: Enable LLM reranking (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
        use_intent_analysis: Enable intent detection
    
    Returns:
        List of (entry, score) tuples where entry has 'chunk' and 'source'
    """
    try:
        # Step 1: Intent Analysis
        intent_data = {}
        if use_intent_analysis:
            try:
                intent_data = _run_async_safely(extract_query_intent(query))
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Intent analysis error: {e}")
        
        # Extract filters from intent
        filters = {}
        for key in ['academic_year', 'semester', 'doc_type']:
            if key in intent_data and intent_data[key] is not None:
                filters[key] = intent_data[key]
        
        if filters:
            logger.info(f"üîç Filters: {filters}")
        
        # Step 2: Hybrid Search
        if use_hybrid and hybrid_retriever.bm25_index is not None:
            # Dense search with filters
            dense_results = vector_manager.search(query, k=k*3, filter_dict=filters)
            
            # Sparse search (BM25) - ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏•‡∏á
            sparse_results = hybrid_retriever.bm25_search(query, k=k*2)
            
            # Apply filters to sparse results
            if filters:
                filtered_sparse = []
                for doc, score in sparse_results:
                    include = True
                    
                    # Filter by doc_type
                    if 'doc_type' in filters:
                        if filters['doc_type'] not in doc.get('source', '').lower():
                            include = False
                    
                    # Filter by academic_year (check in chunk content)
                    if 'academic_year' in filters:
                        if filters['academic_year'] not in doc.get('chunk', ''):
                            include = False
                    
                    if include:
                        filtered_sparse.append((doc, score))
                
                sparse_results = filtered_sparse
            
            # RRF Fusion with lower weight for BM25
            fused_results = hybrid_retriever.rrf_fusion(
                dense_results, 
                sparse_results, 
                k=k*2,  # Get more candidates for reranking
                dense_weight=0.7,  # üÜï ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å dense ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
                sparse_weight=0.3   # üÜï ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å BM25
            )
            
            logger.info(f"üîÄ Hybrid: {len(dense_results)} dense + {len(sparse_results)} sparse ‚Üí {len(fused_results)} fused")
            
        else:
            # Fallback to pure semantic search
            logger.info("üì° Using pure semantic search")
            fused_results = vector_manager.search(query, k=k*2, filter_dict=filters)
        
        # Step 3: Convert to (entry, score) tuples
        scored_chunks = []
        for result in fused_results:
            entry = {
                'chunk': result.get('chunk', ''),
                'source': result.get('source', ''),
                'index': result.get('metadata', {}).get('chunk_index', 0)
            }
            score = result.get('rrf_score', result.get('score', 0))
            scored_chunks.append((entry, score))
        
        # Step 4: LLM Reranking (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!)
        if use_llm_rerank and scored_chunks:
            try:
                scored_chunks = _run_async_safely(
                    llm_rerank_chunks(query, scored_chunks, intent_data, top_k=k)
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è LLM reranking error: {e}")
                scored_chunks = scored_chunks[:k]
        else:
            scored_chunks = scored_chunks[:k]
        
        if not scored_chunks:
            logger.warning(f"‚ö†Ô∏è No results found for query: '{query}'")
        else:
            logger.info(f"‚úÖ Final results: {len(scored_chunks)} chunks (top score: {scored_chunks[0][1]:.2f})")
        
        return scored_chunks

    except Exception as e:
        logger.error(f"‚ùå Retrieval Error: {e}")
        import traceback
        traceback.print_exc()
        return []
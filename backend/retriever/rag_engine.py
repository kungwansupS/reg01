import os
import hashlib
import uuid
from typing import List, Dict, Any

# LangChain & Qdrant Integration
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

# App Config
from app.config import GEMINI_API_KEY, GEMINI_MODEL_NAME, PDF_QUICK_USE_FOLDER, QDRANT_PATH

# ==========================================
# 1. Setup Embedding & Vector DB
# ==========================================

print("‚ö° Initializing RAG Engine...")

# ‡πÉ‡∏ä‡πâ EmbeddingGemma ‡∏ï‡∏≤‡∏° Requirement
embeddings = HuggingFaceEmbeddings(
    model_name="google/embeddinggemma-300M",
    model_kwargs={'device': 'cpu'}, 
    encode_kwargs={'normalize_embeddings': True}
)

# Dynamic Dimension Detection (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Hardcode)
try:
    dummy_vector = embeddings.embed_query("test")
    EMBEDDING_SIZE = len(dummy_vector)
    print(f"   - Embedding Model Loaded (Dim: {EMBEDDING_SIZE})")
except Exception as e:
    print(f"‚ùå Error loading embeddings: {e}")
    EMBEDDING_SIZE = 768 # Fallback

# Setup Qdrant Client (Persistent)
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ folder ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á
if not os.path.exists(QDRANT_PATH):
    os.makedirs(QDRANT_PATH, exist_ok=True)

qdrant_client = QdrantClient(path=QDRANT_PATH)
COLLECTION_NAME = "reg_knowledge_base"

# Create Collection if not exists
if not qdrant_client.collection_exists(COLLECTION_NAME):
    qdrant_client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.COSINE),
    )
    print(f"   - Created new Qdrant collection: {COLLECTION_NAME}")
else:
    print(f"   - Connected to existing Qdrant collection: {COLLECTION_NAME}")

# Connect LangChain to Qdrant
vector_store = QdrantVectorStore(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embedding=embeddings,
)

# ==========================================
# 2. Utility Functions
# ==========================================

def generate_file_hash(filepath: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Hash MD5 ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Unique ID ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

# ==========================================
# 3. Ingestion Logic
# ==========================================

def run_ingestion():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Index ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Vector DB
    ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥:
    - Idempotency: ‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ (‡πÉ‡∏ä‡πâ Hash ID)
    - Metadata: ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå, ID, Index
    - Optimized Chunking: ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏°‡∏µ Overlap
    """
    print(f"\nüöÄ Starting Ingestion Process from: {PDF_QUICK_USE_FOLDER}")
    
    if not os.path.exists(PDF_QUICK_USE_FOLDER):
        print(f"‚ùå Folder not found: {PDF_QUICK_USE_FOLDER}")
        return

    # Scan files manually to control ID generation
    files = []
    for root, _, filenames in os.walk(PDF_QUICK_USE_FOLDER):
        for filename in filenames:
            if filename.lower().endswith(".txt"): # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö .txt (‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÄ‡∏û‡∏¥‡πà‡∏° .pdf ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà)
                files.append(os.path.join(root, filename))
    
    if not files:
        print("‚ö†Ô∏è No text files found to ingest.")
        return

    # Text Splitter Configuration
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""],
        add_start_index=True
    )

    documents_to_upsert = []
    ids_to_upsert = []
    
    print(f"   - Found {len(files)} files. Processing...")

    for filepath in files:
        try:
            # 1. Generate Doc ID based on Content Hash
            file_hash = generate_file_hash(filepath)
            doc_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, file_hash))
            filename = os.path.basename(filepath)

            # 2. Load Text
            loader = TextLoader(filepath, encoding='utf-8')
            raw_docs = loader.load()

            # 3. Split & Inject Metadata
            chunks = text_splitter.split_documents(raw_docs)
            
            for i, chunk in enumerate(chunks):
                # Deterministic Chunk ID: Hash(DocID + Index) -> ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏™‡∏°‡∏≠‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥
                chunk_id = hashlib.md5(f"{doc_uuid}_{i}".encode()).hexdigest()
                
                # Update Metadata
                chunk.metadata["doc_id"] = doc_uuid
                chunk.metadata["file_hash"] = file_hash
                chunk.metadata["original_filename"] = filename
                chunk.metadata["chunk_index"] = i
                
                documents_to_upsert.append(chunk)
                ids_to_upsert.append(chunk_id)
                
        except Exception as e:
            print(f"‚ùå Failed to process {filepath}: {e}")

    # 4. Batch Upsert to Qdrant
    if documents_to_upsert:
        vector_store.add_documents(documents=documents_to_upsert, ids=ids_to_upsert)
        print(f"‚úÖ Successfully upserted {len(documents_to_upsert)} chunks to Qdrant.")
        print(f"   - Storage Path: {QDRANT_PATH}")
    else:
        print("‚ö†Ô∏è No valid chunks generated.")

# ==========================================
# 4. Retrieval Logic (RAG)
# ==========================================

def ask_rag(query: str) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô RAG ‡∏´‡∏•‡∏±‡∏Å
    - ‡πÉ‡∏ä‡πâ MMR ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° Diversity ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    - ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Answer + Citations
    """
    # 1. Setup LLM
    if not GEMINI_API_KEY:
        return {"answer": "‚ùå Error: GEMINI_API_KEY not found in environment.", "citations": []}

    llm = ChatGoogleGenerativeAI(
        model=GEMINI_MODEL_NAME,
        google_api_key=GEMINI_API_KEY,
        temperature=0.3
    )

    # 2. Setup Retriever with MMR (Maximal Marginal Relevance)
    retriever = vector_store.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 5,             # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            "fetch_k": 20,      # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å
            "lambda_mult": 0.5  # Balance: 0.5 (‡∏Å‡∏•‡∏≤‡∏á‡πÜ), 1.0 (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô similarity ‡∏õ‡∏Å‡∏ï‡∏¥)
        }
    )

    # 3. Prompt Template
    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô REG "
        "‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Context ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
        "‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Context ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ' ‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏≠‡∏á\n\n"
        "--- Context ---\n"
        "{context}\n"
        "---------------"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    # 4. Create Chain
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # 5. Invoke & Format Result
    try:
        response = rag_chain.invoke({"input": query})
        
        answer = response["answer"]
        context_docs = response["context"]
        
        # Extract Citations
        citations = []
        seen = set()
        for doc in context_docs:
            fname = doc.metadata.get("original_filename", "unknown")
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô PDF ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ page number (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)
            page = doc.metadata.get("page", None)
            
            ref = fname
            if page is not None:
                ref += f" (Page {page + 1})"
            
            if ref not in seen:
                citations.append(ref)
                seen.add(ref)
        
        return {
            "answer": answer,
            "citations": citations,
            "source_documents": context_docs # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ debug ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        }

    except Exception as e:
        print(f"‚ùå RAG Error: {e}")
        return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", "citations": []}

# Test block (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ï‡∏£‡∏á‡πÜ)
if __name__ == "__main__":
    # Test Ingestion
    run_ingestion()
    
    # Test Query
    test_query = "‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
    print(f"\n‚ùì Query: {test_query}")
    result = ask_rag(test_query)
    print(f"üí° Answer: {result['answer']}")
    print(f"üìö Citations: {result['citations']}")
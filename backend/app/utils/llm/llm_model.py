import os
import openai
from google import genai
from app.config import GEMINI_API_KEY, GEMINI_MODEL_NAME, OPENAI_API_KEY, OPENAI_MODEL_NAME, LLM_PROVIDER

openai.api_key = OPENAI_API_KEY

def get_llm_model():
    if LLM_PROVIDER == "gemini":
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Client ‡∏Ç‡∏≠‡∏á google-genai
        if not GEMINI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Environment Variables")
        return genai.Client(api_key=GEMINI_API_KEY)
    elif LLM_PROVIDER == "openai":
        return openai
    else:
        raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å LLM_PROVIDER: {LLM_PROVIDER}")

def log_llm_usage(response, context="", model_name=None):
    if LLM_PROVIDER == "gemini":
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á usage ‡∏à‡∏≤‡∏Å response ‡∏Ç‡∏≠‡∏á SDK ‡πÉ‡∏´‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        usage = getattr(response, "usage_metadata", None)
        if usage:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö attribute ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á SDK ‡πÉ‡∏´‡∏°‡πà (‡∏≠‡∏≤‡∏à‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏¢)
            prompt_tokens = getattr(usage, "prompt_token_count", 0)
            completion_tokens = getattr(usage, "candidates_token_count", 0)
            total_tokens = getattr(usage, "total_token_count", 0)
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• usage ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô 0
            prompt_tokens = completion_tokens = total_tokens = 0

    elif LLM_PROVIDER == "openai":
        usage = getattr(response, "usage", None)
        if usage:
            prompt_tokens = usage.prompt_tokens
            completion_tokens = usage.completion_tokens
            total_tokens = usage.total_tokens
        else:
            prompt_tokens = completion_tokens = total_tokens = 0

    else:
        prompt_tokens = completion_tokens = total_tokens = 0

    print(
        f"üî¢ {LLM_PROVIDER.capitalize()} token usage ({context}) - "
        f"Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}"
    )
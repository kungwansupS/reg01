import edge_tts
import re
import logging
from typing import AsyncGenerator

logger = logging.getLogger(__name__)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤
LANGUAGE_SETTINGS = {
    "th": {
        # th-TH-NiwatNeural (‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢)
        # th-TH-PremwadeeNeural (‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á)
        "voice": "th-TH-NiwatNeural",
        "rate": "+0%",
        "volume": "+5%",
        "pitch": "+0Hz"
    },
    "en": {
        # en-US-GuyNeural (‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢)
        # en-US-AnaNeural (‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á)
        "voice": "en-US-GuyNeural",
        "rate": "-10%",
        "volume": "+3%",
        "pitch": "+0Hz"
    },
    "zh": {
        # zh-CN-YunxiNeural (‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢)
        # zh-CN-XiaoxiaoNeural (‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á)
        "voice": "zh-CN-YunxiNeural",
        "rate": "-20%",
        "volume": "+5%",
        "pitch": "-20Hz"
    },
    "ja": {
        # ja-JP-KeitaNeural (‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢)
        # ja-JP-NanamiNeural (‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á)
        "voice": "ja-JP-KeitaNeural",
        "rate": "-10%",
        "volume": "+5%",
        "pitch": "+0Hz"
    }
}

async def speak_segment(segment_text: str, settings: dict) -> AsyncGenerator[bytes, None]:
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö segment ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    
    Args:
        segment_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
        settings: ‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á (voice, rate, volume, pitch)
    
    Yields:
        bytes: Audio data chunks
    """
    try:
        communicate = edge_tts.Communicate(
            text=segment_text,
            voice=settings["voice"],
            rate=settings["rate"],
            volume=settings["volume"],
            pitch=settings["pitch"]
        )
        
        chunk_count = 0
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                chunk_count += 1
                yield chunk["data"]
        
        logger.debug(f"‚úÖ Generated {chunk_count} audio chunks for: '{segment_text[:30]}...'")
        
    except Exception as e:
        logger.error(f"‚ùå TTS Error for '{segment_text[:30]}...': {e}")
        # ‡∏™‡πà‡∏á silent audio ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ stream ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        yield b'\x00' * 1024

async def speak(text: str) -> AsyncGenerator[bytes, None]:
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞ streaming
    
    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á
    
    Yields:
        bytes: Audio data chunks (MP3 format)
    
    Example:
        >>> async for chunk in speak("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö // Hello"):
        ...     # ‡∏™‡πà‡∏á chunk ‡πÑ‡∏õ‡∏¢‡∏±‡∏á client
    """
    if not text or not text.strip():
        logger.warning("‚ö†Ô∏è Empty text provided to TTS")
        yield b'\x00' * 1024
        return
    
    try:
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text = preprocess_text(text)
        
        # ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏≤‡∏° // delimiter
        parts = [p.strip() for p in text.split("//") if p.strip()]
        
        if not parts:
            logger.warning("‚ö†Ô∏è No valid parts after preprocessing")
            yield b'\x00' * 1024
            return
        
        logger.info(f"üéôÔ∏è Speaking {len(parts)} parts")
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô
        for i, part in enumerate(parts):
            # ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤
            segments = split_text_by_language(part)
            
            logger.debug(f"Part {i+1}/{len(parts)}: {len(segments)} language segments")
            
            # ‡∏û‡∏π‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ segment
            for lang, segment_text in segments:
                if not segment_text.strip():
                    continue
                    
                settings = LANGUAGE_SETTINGS.get(lang, LANGUAGE_SETTINGS["th"])
                
                async for chunk in speak_segment(segment_text, settings):
                    yield chunk
            
            # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á parts (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ parts)
            if i < len(parts) - 1:
                # ‡∏™‡πà‡∏á silence ‡∏™‡∏±‡πâ‡∏ô‡πÜ (0.2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
                yield b'\x00' * 512
        
        logger.info("‚úÖ TTS completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Critical TTS Error: {e}")
        yield b'\x00' * 1024

def preprocess_text(text: str) -> str:
    """
    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á
    
    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    
    Returns:
        str: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
    """
    # ‡∏•‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö (‡∏õ‡∏Å‡∏ï‡∏¥‡πÄ‡∏õ‡πá‡∏ô metadata)
    text = re.sub(r'\([^)]*\)', '', text)
    
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ/URL
    replacements = {
        # URL components
        r'(?<!\S)www(?=\.)': "world wide web",
        r'\.com\b': "dot com",
        r'\.org\b': "dot org",
        r'\.net\b': "dot net",
        r'\.ac\b': "dot A C",
        r'\.th\b': "dot T H",
        r'\.co\b': "dot C O",
        r'(?<!\S)cmu(?=\.ac\.th)': "C M U",
        
        # Abbreviations
        r'\.e\.g\.\b': "for example",
        r'\.i\.e\.\b': "that is",
        r'\.dept\.\b': "department",
        r'\betc\.\b': "et cetera",
        
        # Common tech terms
        r'\bAPI\b': "A P I",
        r'\bURL\b': "U R L",
        r'\bPDF\b': "P D F",
        r'\bHTML\b': "H T M L",
        
        # Remove [Bot ‡∏û‡∏µ‡πà‡πÄ‡∏£‡πá‡∏Å] prefix
        r'^\[Bot ‡∏û‡∏µ‡πà‡πÄ‡∏£‡πá‡∏Å\]\s*': '',
        r'^\[Admin\]:\s*': '',
    }
    
    for pattern, replacement in replacements.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def split_text_by_language(text: str):
    """
    ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    
    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å
    
    Returns:
        list: List of tuples (language_code, text_segment)
        
    Example:
        >>> split_text_by_language("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ Hello ‰Ω†Â•Ω")
        [('th', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ '), ('en', 'Hello '), ('zh', '‰Ω†Â•Ω')]
    """
    # Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ
    pattern = r'([‡∏Å-‡πô]+|[a-zA-Z]+|[0-9]+|[.,!?\'"() ]+|[\u4e00-\u9fff]+|[\u3040-\u309F\u30A0-\u30FF]+)'
    matches = re.finditer(pattern, text)

    segments = []
    current_lang = None
    current_text = ""

    for match in matches:
        segment = match.group()
        
        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        if not segment.strip():
            current_text += segment
            continue

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏†‡∏≤‡∏©‡∏≤
        if re.match(r'^[0-9]+$', segment):
            # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç - ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            lang = current_lang if current_lang else "th"
        elif re.search(r'[‡∏Å-‡πô]', segment):
            # ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            lang = "th"
        elif re.search(r'[a-zA-Z]', segment):
            # ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
            lang = "en"
        elif re.search(r'[\u4e00-\u9fff]', segment):
            # ‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô
            lang = "zh"
        elif re.search(r'[\u3040-\u309F\u30A0-\u30FF]', segment):
            # ‡∏†‡∏≤‡∏©‡∏≤‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô
            lang = "ja"
        else:
            # ‡∏≠‡∏∑‡πà‡∏ô‡πÜ - ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            lang = current_lang if current_lang else "th"

        # ‡∏£‡∏ß‡∏° segment ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        if lang == current_lang or current_lang is None:
            current_text += segment
            current_lang = lang
        else:
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å segment ‡πÄ‡∏Å‡πà‡∏≤
            if current_text.strip():
                segments.append((current_lang, current_text.strip()))
            # ‡πÄ‡∏£‡∏¥‡πà‡∏° segment ‡πÉ‡∏´‡∏°‡πà
            current_text = segment
            current_lang = lang

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å segment ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    if current_text.strip():
        segments.append((current_lang, current_text.strip()))

    # Fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ segments
    if not segments:
        segments = [("th", text)]

    return segments

# ----------------------------------------------------------------------------- #
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
# ----------------------------------------------------------------------------- #
async def test_tts():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö TTS"""
    test_texts = [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö",
        "Hello world",
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö // Hello // ‰Ω†Â•Ω",
        "‡∏õ‡∏è‡∏¥‡∏ó‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ Academic Calendar 2568",
        "",  # Edge case: empty
        "www.cmu.ac.th ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà"
    ]
    
    for text in test_texts:
        print(f"\n{'='*60}")
        print(f"Testing: {text[:50]}")
        print('='*60)
        
        chunk_count = 0
        async for chunk in speak(text):
            chunk_count += 1
        
        print(f"‚úÖ Generated {chunk_count} chunks")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_tts())
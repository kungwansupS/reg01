import os
import subprocess
import time
import logging
import httpx
import openai
import shutil
from google import genai
from app.config import (
    GEMINI_API_KEY,
    OPENAI_API_KEY,
    LLM_PROVIDER,
    OPENAI_BASE_URL,
    LOCAL_API_KEY,
    LOCAL_BASE_URL,
    LOCAL_MODEL_NAME
)

logger = logging.getLogger(__name__)

def ensure_local_llm_ready():
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á Ollama ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç WinError 2)
    """
    if LLM_PROVIDER != "local":
        return

    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Ollama ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    ollama_path = shutil.which("ollama")
    if not ollama_path:
        logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° 'ollama' ‡πÉ‡∏ô System PATH ‡πÇ‡∏õ‡∏£‡∏î‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å https://ollama.com")
        return

    base_url_only = LOCAL_BASE_URL.replace("/v1", "")
    
    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Service ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    try:
        with httpx.Client() as client:
            client.get(base_url_only, timeout=2.0)
    except Exception:
        logger.info("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Ollama Service...")
        try:
            if os.name == 'nt': # Windows
                subprocess.Popen(["ollama", "serve"], creationflags=subprocess.CREATE_NEW_CONSOLE)
            else: # Linux/Mac
                subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            time.sleep(3) # ‡∏£‡∏≠‡πÉ‡∏´‡πâ Service ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡∏±‡∏ß
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î Ollama ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÑ‡∏î‡πâ: {e}")
            return

    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ Pull ‡πÇ‡∏°‡πÄ‡∏î‡∏•
    try:
        with httpx.Client(timeout=10.0) as client:
            tags_res = client.get(f"{base_url_only}/api/tags")
            models = [m['name'] for m in tags_res.json().get('models', [])]
            
            if LOCAL_MODEL_NAME not in models and f"{LOCAL_MODEL_NAME}:latest" not in models:
                logger.info(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• {LOCAL_MODEL_NAME} (‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ô‡∏≤‡∏ó‡∏µ)...")
                # ‡πÉ‡∏ä‡πâ shell=True ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Windows ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ Path
                subprocess.run(["ollama", "pull", LOCAL_MODEL_NAME], shell=(os.name == 'nt'), check=True)
                logger.info(f"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• {LOCAL_MODEL_NAME} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è ‡∏Å‡∏≤‡∏£ Pull ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á: {e}")

def get_llm_model():
    if LLM_PROVIDER == "gemini":
        if not GEMINI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY")
        return genai.Client(api_key=GEMINI_API_KEY)

    elif LLM_PROVIDER == "openai":
        if not OPENAI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö OPENAI_API_KEY")
        return openai.OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

    elif LLM_PROVIDER == "local":
        ensure_local_llm_ready()
        return openai.OpenAI(api_key=LOCAL_API_KEY, base_url=LOCAL_BASE_URL)
    
    raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å LLM_PROVIDER: {LLM_PROVIDER}")

def log_llm_usage(response, context="", model_name=None):
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    try:
        if LLM_PROVIDER == "gemini":
            usage = getattr(response, "usage_metadata", None)
            if usage:
                prompt_tokens = usage.prompt_token_count
                completion_tokens = usage.candidates_token_count
                total_tokens = usage.total_token_count
        elif LLM_PROVIDER in ["openai", "local"]:
            usage = getattr(response, "usage", None)
            if usage:
                prompt_tokens = usage.prompt_tokens
                completion_tokens = usage.completion_tokens
                total_tokens = usage.total_tokens
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading usage logs: {e}")
    print(f"üî¢ {LLM_PROVIDER.capitalize()} usage ({context}) - Total: {total_tokens}")
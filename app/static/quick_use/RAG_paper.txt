===================

**RAG (Retrieval-Augmented Generation) Paper Summary**

*   **Title:** Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
*   **Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela
*   **Affiliations:** Facebook AI Research, University College London, New York University
*   **Abstract:** Introduces RAG models, combining pre-trained parametric and non-parametric memory for language generation. Uses Wikipedia as non-parametric memory accessed via a neural retriever. Achieves state-of-the-art on open domain QA and generates more specific, diverse, and factual language.

===================

**Introduction**

*   **Problem:** Large language models store factual knowledge but have limitations in accessing and manipulating it, providing provenance, and updating knowledge. They also may produce "hallucinations".
*   **Solution:** Hybrid models combining parametric and non-parametric memory.
*   **RAG Approach:** Fine-tuning pre-trained seq2seq models with a non-parametric memory (Wikipedia) accessed via a neural retriever.
*   **Benefits:** Knowledge can be directly revised, expanded, inspected, and interpreted.
*   **RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.**

===================

**Methods**

*   **RAG Models:** Use input sequence x to retrieve text documents z and use them as additional context for generating the target sequence y.
*   **Components:**
    *   Retriever pη(z|x): Returns distributions over text passages given a query x.
    *   Generator pθ(yi|x, z, y1:i−1): Generates current token based on previous tokens, input x, and retrieved passage z.
*   **Training:** Treat retrieved document as a latent variable and marginalize over latent documents.
*   **RAG-Sequence:** Uses the same retrieved document to generate the complete sequence.
*   **RAG-Token:** Can predict each target token based on a different document.

===================

**Retriever: DPR (Dense Passage Retriever)**

*   **Architecture:** Bi-encoder architecture
    *   pη(z|x) ∝exp(d(z)⊤q(x))
    *   d(z) = BERTd(z), q(x) = BERTq(x)
*   **Implementation:** Uses a pre-trained bi-encoder from DPR trained on TriviaQA and Natural Questions.
*   **Non-parametric Memory:** Document index.

===================

**Generator: BART**

*   **Model:** BART-large (400M parameters), a pre-trained seq2seq transformer.
*   **Combination:** Concatenate input x with retrieved content z when generating from BART.
*   **Parametric Memory:** BART generator parameters θ.

===================

**Training Details**

*   Jointly train retriever and generator without direct supervision on retrieved documents.
*   Minimize negative marginal log-likelihood of each target using stochastic gradient descent with Adam.
*   Document encoder (BERTd) is kept fixed during training; only the query encoder (BERTq) and BART generator are fine-tuned.

===================

**Decoding**

*   **RAG-Token:** Standard autoregressive seq2seq generator with modified transition probability.  Beam decoder used.
*   **RAG-Sequence:**
    *   Cannot be solved with a single beam search. Run beam search for each document z.
    *   Thorough Decoding: Estimate probability of an hypothesis y by running forward passes for each document z.
    *   Fast Decoding: Approximate pθ(y|x, zi) ≈ 0 if y was not generated during beam search from x, zi.

===================

**Experiments**

*   **Tasks:** Knowledge-intensive tasks.
*   **Knowledge Source:** Single Wikipedia dump (December 2018), split into 100-word chunks (21M documents).
*   **MIPS Index:** FAISS with Hierarchical Navigable Small World approximation.
*   **Retrieval:** Top k documents for each query (k ∈ {5, 10} for training).

===================

**Evaluation Tasks**

*   **Open-domain Question Answering (QA):**
    *   Datasets: Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), CuratedTrec (CT).
    *   Evaluation Metric: Exact Match (EM) scores.
*   **Abstractive Question Answering:**
    *   Dataset: MSMARCO NLG task v2.1.
*   **Jeopardy Question Generation:**
    *   Dataset: SearchQA splits.
    *   Evaluation Metric: SQuAD-tuned Q-BLEU-1, human evaluations (factuality, specificity).
*   **Fact Verification:**
    *   Dataset: FEVER.
    *   Evaluation Metric: Label accuracy (3-way and 2-way classification).

===================

**Results: Open-domain QA**

*   RAG sets a new state of the art on all four open-domain QA tasks.
*   Combines generation flexibility of "closed-book" approaches with performance of retrieval-based approaches.
*   RAG retriever initialized using DPR's retriever.

===================

**Results: Abstractive Question Answering**

*   RAG-Sequence outperforms BART on Open MS-MARCO NLG.
*   RAG models hallucinate less and generate more factually correct text.

===================

**Results: Jeopardy Question Generation**

*   RAG-Token performs better than RAG-Sequence and BART on Q-BLEU-1.
*   Human evaluations: RAG is more factual and specific than BART.
*   Parametric and non-parametric memories work together.

===================

**Results: Fact Verification**

*   RAG scores within 4.3% of state-of-the-art models on FEVER (3-way classification) without retrieval supervision.
*   Achieves accuracy within 2.7% of a model trained with gold evidence sentences on FEVER (2-way classification).
*   71% of top retrieved documents are from a gold article, 90% within top 10.

===================

**Additional Results**

*   **Generation Diversity:** RAG-Sequence's generations are more diverse than RAG-Token's, and both are more diverse than BART.
*   **Retrieval Ablations:** Learned retrieval improves results for all tasks. Differentiable retrieval superior to BM25 except on FEVER.
*   **Index Hot-Swapping:** Replacing the non-parametric memory updates RAG's world knowledge.
*   **Effect of Retrieving more documents:** Retrieving more documents at test time improves Open-domain QA results for RAG-Sequence (performance peaks for RAG-Token at 10 documents).

===================

**Related Work**

*   Single-Task Retrieval
*   General-Purpose Architectures for NLP
*   Learned Retrieval
*   Memory-based Architectures
*   Retrieve-and-Edit approaches

===================

**Discussion**

*   RAG models obtain state-of-the-art results on open-domain QA.
*   People prefer RAG's generation over purely parametric BART.
*   Learned retrieval component is effective.
*   Retrieval index can be hot-swapped to update the model.

===================

**Broader Impact**

*   Offers positive societal benefits over previous work (more factual generations, more control and interpretability).
*   Potential downsides: Biases and inaccuracies from external knowledge sources (Wikipedia), potential misuse for generating misleading content.

===================

**Appendices**

*   **A: Implementation Details**
*   **B: Human Evaluation**
*   **C: Training setup Details**
*   **D: Further Details on Open-Domain QA**
*   **E: Further Details on FEVER**
*   **F: Null Document Probabilities**
*   **G: Parameters**
*   **H: Retrieval Collapse**
*   **I: Number of instances per dataset**

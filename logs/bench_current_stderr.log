INFO:pdf_to_txt:\u2705 Gemini client initialized
C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\pydub\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
  warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
INFO:root:\U0001f680 Starting REG-01 Backend...
INFO:root:\U0001f4e1 Server running at 0.0.0.0:5000
INFO:     Started server process [27056]
INFO:     Waiting for application startup.
INFO:MainBackend:\U0001f680 Starting REG-01 Application...
INFO:BackgroundTasks:\U0001f680 [Startup RAG] Begin pipeline | process_pdf=True build_hybrid=True
INFO:BackgroundTasks:\U0001f4c4 [Startup RAG] Running PDF to TXT pipeline...
INFO:pdf_to_txt:\U0001f527 Rate Limiter initialized for: gemini
INFO:pdf_to_txt:   TPM Limit: 13,000
INFO:pdf_to_txt:   RPM Limit: 25
INFO:pdf_to_txt:   Batch Size: 1
INFO:pdf_to_txt:   Batch Delay: 1.0s
INFO:pdf_to_txt:\U0001f50d Scanning folder: D:\github\reg01\backend\app\static/docs
INFO:pdf_to_txt:\U0001f4ca Total PDF files detected: 2
INFO:pdf_to_txt:\u2b50\ufe0f Skipping: REG_payment.pdf (unchanged)
INFO:pdf_to_txt:\u2b50\ufe0f Skipping: calendar/CMU-Academic-Calendar-2568.pdf (unchanged)
INFO:pdf_to_txt:\u2705 All files are up to date
INFO:BackgroundTasks:\u2705 [Startup RAG] PDF to TXT completed in 0.01s
INFO:BackgroundTasks:\U0001f50d [Vector DB] Starting startup synchronization...
INFO:BackgroundTasks:\u2705 [Vector DB] Database is already up-to-date.
INFO:BackgroundTasks:\U0001f528 [Hybrid] Building BM25 index...
INFO:VectorManager:\U0001f4da Retrieved 22 chunks for indexing
INFO:HybridRetriever:\u2705 BM25 index built with 22 documents
INFO:BackgroundTasks:\u2705 [Hybrid] BM25 index ready with 22 chunks
INFO:BackgroundTasks:\u2705 [Startup RAG] Pipeline completed in 0.08s
INFO:MainBackend:\u2705 Application ready
INFO:memory.session_db:\U0001f9f9 Cleaned up 0 old sessions
INFO:memory.session:\U0001f9f9 Cleaned up 0 old sessions
INFO:BackgroundTasks:\U0001f9f9 Maintenance: Old sessions cleaned up
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)
INFO:     127.0.0.1:56463 - "WebSocket /socket.io/?EIO=4&transport=websocket&sid=UEpyuDfImLDj3LFvAAAA" [accepted]
INFO:     connection open
INFO:     127.0.0.1:51417 - "WebSocket /socket.io/?EIO=4&transport=websocket&sid=ibA8n1_2-kH54irMAAAB" [accepted]
INFO:     connection open
INFO:     127.0.0.1:59367 - "WebSocket /socket.io/?EIO=4&transport=websocket" [accepted]
INFO:     connection open
INFO:SocketIOHandlers:Registered sid=-_td_mkgbU3FDVskAAAF to room=web_session:08c8a3b3-48fb-451b-90c6-19a8b633c688
WARNING:app.utils.token_counter:Model openai/gpt-oss-120b not found, using cl100k_base encoding
INFO:app.utils.llm.llm_model:OpenAI key #1 ready (gsk_vg...6GvG)
INFO:app.utils.llm.llm_model:OpenAI key #2 ready (gsk_JM...0VPG)
INFO:app.utils.llm.llm_model:OpenAI failover enabled with 2 keys.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:app.utils.llm.llm_model:OpenAI failover switched active key to #2 (gsk_JM...0VPG).
INFO:app.utils.llm.llm:[OPENAI Call 1] Tokens - Prompt: 1,164 | Completion: 161 | Total: 1,325
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (APIConnectionError, status=none); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (APIConnectionError, status=none); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:ContextSelector:\U0001f3af Intent: date_query, Type: date
INFO:ContextSelector:\U0001f511 Key entities: ['วันเปิดภาคการศึกษา', 'ภาคเรียนที่ 1', 'ปีการศึกษา 2568', 'มช']
INFO:ContextSelector:\U0001f50d Filters: {'academic_year': '2568', 'semester': 1, 'doc_type': 'calendar'}
INFO:VectorManager:Loading embedding model: BAAI/bge-m3 on cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|##########| 1/1 [00:00<00:00, 27.07it/s]
INFO:VectorManager:\u2705 Post-filtered: 16 \u2192 15 results
INFO:HybridRetriever:\U0001f500 RRF fusion (dense: 70.0%, sparse: 30.0%): 15 + 10 \u2192 10 results
INFO:ContextSelector:\U0001f500 Hybrid: 15 dense + 10 sparse \u2192 10 fused
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (APIConnectionError, status=none); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:ContextSelector:\U0001f3af LLM Reranked: 10 chunks, top score: 0.92
INFO:ContextSelector:\u2705 Final results: 5 chunks (top score: 0.92)
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (APIConnectionError, status=none); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (APIConnectionError, status=none); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm:[OPENAI Call 2 RAG] Tokens - Prompt: 4,842 | Completion: 174 | Total: 5,016
INFO:app.utils.llm.llm:[Total Token Usage] Tokens - Prompt: 6,006 | Completion: 335 | Total: 6,341
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 562 tokens
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI transient errors persisted across failover rounds (2 rounds).
ERROR:app.utils.llm.llm:LLM Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199724, Requested 1165. Please try again in 6m24.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "D:\github\reg01\backend\app\utils\llm\llm.py", line 481, in ask_llm
    response = await model.chat.completions.create(
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 112, in create
    return await self._pool.create_chat_completion(*args, **kwargs)
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 176, in create_chat_completion
    raise last_retryable_error
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 143, in create_chat_completion
    result = await client.chat.completions.create(*args, **kwargs)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1797, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1597, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199724, Requested 1165. Please try again in 6m24.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 405 tokens
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI transient errors persisted across failover rounds (2 rounds).
ERROR:app.utils.llm.llm:LLM Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199719, Requested 1177. Please try again in 6m27.071999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "D:\github\reg01\backend\app\utils\llm\llm.py", line 481, in ask_llm
    response = await model.chat.completions.create(
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 112, in create
    return await self._pool.create_chat_completion(*args, **kwargs)
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 176, in create_chat_completion
    raise last_retryable_error
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 143, in create_chat_completion
    result = await client.chat.completions.create(*args, **kwargs)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1797, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1597, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199719, Requested 1177. Please try again in 6m27.071999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 429 tokens
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI transient errors persisted across failover rounds (2 rounds).
ERROR:app.utils.llm.llm:LLM Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199712, Requested 1159. Please try again in 6m16.272s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "D:\github\reg01\backend\app\utils\llm\llm.py", line 481, in ask_llm
    response = await model.chat.completions.create(
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 112, in create
    return await self._pool.create_chat_completion(*args, **kwargs)
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 176, in create_chat_completion
    raise last_retryable_error
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 143, in create_chat_completion
    result = await client.chat.completions.create(*args, **kwargs)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1797, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1597, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199712, Requested 1159. Please try again in 6m16.272s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 434 tokens
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI transient errors persisted across failover rounds (2 rounds).
ERROR:app.utils.llm.llm:LLM Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199706, Requested 1174. Please try again in 6m20.16s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "D:\github\reg01\backend\app\utils\llm\llm.py", line 481, in ask_llm
    response = await model.chat.completions.create(
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 112, in create
    return await self._pool.create_chat_completion(*args, **kwargs)
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 176, in create_chat_completion
    raise last_retryable_error
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 143, in create_chat_completion
    result = await client.chat.completions.create(*args, **kwargs)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1797, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1597, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199706, Requested 1174. Please try again in 6m20.16s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 429 tokens
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI transient errors persisted across failover rounds (2 rounds).
ERROR:app.utils.llm.llm:LLM Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199699, Requested 1159. Please try again in 6m10.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "D:\github\reg01\backend\app\utils\llm\llm.py", line 481, in ask_llm
    response = await model.chat.completions.create(
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 112, in create
    return await self._pool.create_chat_completion(*args, **kwargs)
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 176, in create_chat_completion
    raise last_retryable_error
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 143, in create_chat_completion
    result = await client.chat.completions.create(*args, **kwargs)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1797, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1597, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199699, Requested 1159. Please try again in 6m10.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:app.utils.llm.llm_model:OpenAI failover switched active key to #1 (gsk_vg...6GvG).
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 456 tokens
INFO:app.utils.llm.llm:[FAQ HIT] matched=ชำระด้วยบัตรเครดิตที่กองคลังได้ถึงกี่โมง score=1.0
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:app.utils.llm.llm_model:OpenAI failover switched active key to #2 (gsk_JM...0VPG).
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 385 tokens
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #2 (gsk_JM...0VPG) transient error (RateLimitError, status=429); trying next key.
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
WARNING:app.utils.llm.llm_model:OpenAI key #1 (gsk_vg...6GvG) transient error (RateLimitError, status=429); trying next key.
WARNING:app.utils.llm.llm_model:OpenAI transient errors persisted across failover rounds (2 rounds).
ERROR:app.utils.llm.llm:LLM Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199992, Requested 1135. Please try again in 8m6.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "D:\github\reg01\backend\app\utils\llm\llm.py", line 481, in ask_llm
    response = await model.chat.completions.create(
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 112, in create
    return await self._pool.create_chat_completion(*args, **kwargs)
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 176, in create_chat_completion
    raise last_retryable_error
  File "D:\github\reg01\backend\app\utils\llm\llm_model.py", line 143, in create_chat_completion
    result = await client.chat.completions.create(*args, **kwargs)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1797, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\skjav\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1597, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kd8k8pp1es7vd6pvb5ke1x7k` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199992, Requested 1135. Please try again in 8m6.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
INFO:app.utils.llm.llm_model:\U0001f522 OPENAI Usage (suggest_pose) - Total: 366 tokens
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
INFO:memory.session_db:\u2705 Database initialized at D:\github\reg01\backend\memory\sessions.db
forrtl: error (200): program aborting due to window-CLOSE event
Image              PC                Routine            Line        Source             
KERNELBASE.dll     00007FFECAC5D64D  Unknown               Unknown  Unknown
KERNEL32.DLL       00007FFECC55E8D7  Unknown               Unknown  Unknown
ntdll.dll          00007FFECD9EC53C  Unknown               Unknown  Unknown

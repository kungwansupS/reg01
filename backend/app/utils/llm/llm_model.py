import os
import openai
from google import genai
from app.config import (
    GEMINI_API_KEY, GEMINI_MODEL_NAME,
    OPENAI_API_KEY, OPENAI_MODEL_NAME,
    LLM_PROVIDER, OPENAI_BASE_URL # import ‡πÄ‡∏û‡∏¥‡πà‡∏°
)

def get_llm_model():
    if LLM_PROVIDER == "gemini":
        if not GEMINI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Environment Variables")
        return genai.Client(api_key=GEMINI_API_KEY)

    elif LLM_PROVIDER == "openai":
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Client ‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏ base_url (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á Groq, DeepSeek, OpenAI)
        if not OPENAI_API_KEY:
            raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö OPENAI_API_KEY ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Groq/OpenAI")

        return openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
    else:
        raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å LLM_PROVIDER: {LLM_PROVIDER}")

def log_llm_usage(response, context="", model_name=None):
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0

    if LLM_PROVIDER == "gemini":
        usage = getattr(response, "usage_metadata", None)
        if usage:
            prompt_tokens = usage.prompt_token_count
            completion_tokens = usage.candidates_token_count
            total_tokens = usage.total_token_count

    elif LLM_PROVIDER == "openai":
        usage = getattr(response, "usage", None)
        if usage:
            prompt_tokens = usage.prompt_tokens
            completion_tokens = usage.completion_tokens
            total_tokens = usage.total_tokens

    print(
        f"üî¢ {LLM_PROVIDER.capitalize()} token usage ({context}) - "
        f"Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}"
    )
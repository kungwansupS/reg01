import logging
from langdetect import detect
from app.utils.llm.llm_model import get_llm_model, log_llm_usage
from app.prompt.prompt import context_prompt
from app.prompt.request_prompt import get_request_prompt
from memory.memory import qa_cache, summarize_chat_history
from memory.faq_cache import update_faq, get_faq_answer
from memory.session import get_or_create_history, save_history
from retriever.context_selector import retrieve_top_k_chunks
from app.config import PDF_QUICK_USE_FOLDER, LLM_PROVIDER, OPENAI_MODEL_NAME, GEMINI_MODEL_NAME

logger = logging.getLogger(__name__)

async def ask_llm(msg, session_id, emit_fn=None):
    detected_lang = detect(msg)
    request_prompt = get_request_prompt(detected_lang)

    if emit_fn:
        await emit_fn("ai_status", {"status": "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."})

    # ------------------------------------------------------------------
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö FAQ ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Context)
    # ------------------------------------------------------------------
    faq_answer = get_faq_answer(msg)
    faq_context_section = ""

    if faq_answer:
        print(f"üéØ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô FAQ: {faq_answer[:50]}...")
        if emit_fn:
            await emit_fn("ai_status", {"status": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å FAQ..."})

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Context Section ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö FAQ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡πâ LLM
        faq_context_section = f"""
        [‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• FAQ (‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢)]
        ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏Ñ‡∏∑‡∏≠: "{faq_answer}"
        
        ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ" ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        ‡πÅ‡∏ï‡πà‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏´‡πâ‡∏ß‡∏ô‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó "‡∏û‡∏µ‡πà‡πÄ‡∏£‡πá‡∏Å" ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        """

    # ------------------------------------------------------------------
    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö QA Cache (Memory Cache - Exact Match)
    # ------------------------------------------------------------------
    if msg in qa_cache:
        logger.info("üß† ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å cache")
        if emit_fn:
            await emit_fn("ai_status", {"status": "‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (cache)"} )

        # [‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] Return ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ main.py error
        return {
            "text": qa_cache[msg],
            "from_faq": False
        }

    # ------------------------------------------------------------------
    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° History ‡πÅ‡∏•‡∏∞ Context
    # ------------------------------------------------------------------
    history = get_or_create_history(session_id)
    if not (history and history[-1]["role"] == "user" and history[-1]["parts"][0]["text"] == msg):
        history.append({"role": "user", "parts": [{"text": msg}]})
        save_history(session_id, history)

    summary = summarize_chat_history(history[:-10])
    history_text = "\n".join([
        f"{turn['role']}: {turn['parts'][0]['text']}" for turn in history[-10:]
    ])

    # ------------------------------------------------------------------
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Prompt (‡∏£‡∏ß‡∏° FAQ Context ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    # ------------------------------------------------------------------
    full_prompt = f"""
        {context_prompt}

        [‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏î‡∏¥‡∏° (Memory ‡∏¢‡πà‡∏≠)]
        {summary}

        {faq_context_section}

        [‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ 10 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î]
        {history_text}

        ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏û‡∏µ‡πà‡πÄ‡∏£‡πá‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏û‡∏π‡∏î ‡∏Ñ‡∏∑‡∏≠: \"{msg}\"
        ‡∏û‡∏µ‡πà‡πÄ‡∏£‡πá‡∏Å‡∏Ñ‡∏ß‡∏£‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢
    """

    if emit_fn:
        await emit_fn("ai_status", {"status": "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."})

    model = get_llm_model()

    # ------------------------------------------------------------------
    # 5. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô LLM
    # ------------------------------------------------------------------
    if LLM_PROVIDER == "gemini":
        response = model.models.generate_content(
            model=GEMINI_MODEL_NAME,
            contents=full_prompt
        )
        reply = response.text.strip() if response.text else ""
    elif LLM_PROVIDER == "openai":
        response = model.chat.completions.create(
            model=OPENAI_MODEL_NAME,
            messages=[{"role": "user", "content": full_prompt}],
        )
        reply = response.choices[0].message.content.strip()
    else:
        raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å LLM_PROVIDER: {LLM_PROVIDER}")

    log_llm_usage(response, context="ask_llm - generate")

    # ------------------------------------------------------------------
    # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö RAG ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
    # ------------------------------------------------------------------
    if "query_request" in reply:
        logger.debug(reply)
        search_query = reply.split("query_request", 1)[1].strip()
        logger.info(f"üîé ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: {search_query}")

        top_chunks = retrieve_top_k_chunks(search_query, k=5, folder=PDF_QUICK_USE_FOLDER)
        context = "\n\n===================\n\n".join([entry['chunk'] for entry, _ in top_chunks])
        prompt_for_answer = request_prompt.format(question=search_query, context=context)

        if LLM_PROVIDER == "gemini":
            response = model.models.generate_content(
                model=GEMINI_MODEL_NAME,
                contents=prompt_for_answer
            )
            reply = response.text.strip() if response.text else ""
        elif LLM_PROVIDER == "openai":
            response = model.chat.completions.create(
                model=OPENAI_MODEL_NAME,
                messages=[{"role": "user", "content": prompt_for_answer}],
            )
            reply = response.choices[0].message.content.strip()
        else:
            raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å LLM_PROVIDER: {LLM_PROVIDER}")

        log_llm_usage(response, context="rag-final-response")

        if emit_fn:
            await emit_fn("selected_context", {
                "text": context[:3000]
            })

        qa_cache[msg] = reply
        update_faq(msg, reply)

    else:
        reply = reply.replace("model:", "").strip()
        qa_cache[msg] = reply

    # ------------------------------------------------------------------
    # 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤
    # ------------------------------------------------------------------
    history.append({"role": "model", "parts": [{"text": reply}]})
    save_history(session_id, history)

    return {
        "text": reply,
        "from_faq": bool(faq_answer)
    }
import os
import sqlite3
import hashlib
import logging
import datetime
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import torch
from typing import List, Dict, Optional

logger = logging.getLogger("VectorManager")

class VectorManager:
    def __init__(self):
        # à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Path à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        self.db_dir = os.path.join("data", "db")
        os.makedirs(self.db_dir, exist_ok=True)
        
        self.sqlite_path = os.path.join(self.db_dir, "metadata.db")
        self.chroma_path = os.path.join(self.db_dir, "chroma_data")
        
        # à¹€à¸•à¸£à¸µà¸¢à¸¡ Device à¸ªà¸³à¸«à¸£à¸±à¸š Embedding
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model_name = "BAAI/bge-m3" if self.device == 'cuda' else "intfloat/multilingual-e5-small"
        self._model = None # Lazy load
        
        # à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­ Database
        self._init_sqlite()
        self.chroma_client = chromadb.PersistentClient(path=self.chroma_path)
        self.collection = self.chroma_client.get_or_create_collection(name="reg_context")

    @property
    def model(self):
        if self._model is None:
            logger.info(f"Loading embedding model: {self.model_name} on {self.device}")
            self._model = SentenceTransformer(self.model_name, device=self.device)
        return self._model

    def _init_sqlite(self):
        """à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸²à¸£à¸²à¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸š File Hash à¹€à¸à¸·à¹ˆà¸­à¹€à¸Šà¹‡à¸„à¸„à¸§à¸²à¸¡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡"""
        with sqlite3.connect(self.sqlite_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS file_registry (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT,
                    last_updated DATETIME
                )
            """)
            conn.commit()

    def get_file_hash(self, filepath):
        """à¸„à¸³à¸™à¸§à¸“ SHA-256 à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œ"""
        hasher = hashlib.sha256()
        with open(filepath, 'rb') as f:
            buf = f.read()
            hasher.update(buf)
        return hasher.hexdigest()

    def needs_update(self, filepath):
        """à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¸•à¹‰à¸­à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¸¥à¸‡ DB à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ"""
        current_hash = self.get_file_hash(filepath)
        with sqlite3.connect(self.sqlite_path) as conn:
            cursor = conn.execute("SELECT file_hash FROM file_registry WHERE file_path = ?", (filepath,))
            row = cursor.fetchone()
            if row is None or row[0] != current_hash:
                return True, current_hash
        return False, current_hash

    def update_registry(self, filepath, file_hash):
        """à¸šà¸±à¸™à¸—à¸¶à¸à¸›à¸£à¸°à¸§à¸±à¸•à¸´à¸à¸²à¸£à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ"""
        with sqlite3.connect(self.sqlite_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO file_registry (file_path, file_hash, last_updated) VALUES (?, ?, ?)",
                (filepath, file_hash, datetime.datetime.now())
            )
            conn.commit()

    def add_document(self, filepath: str, chunks: List[str], metadata: Optional[Dict] = None):
        """
        à¸™à¸³ Chunks à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¹€à¸‚à¹‰à¸²à¸ªà¸¹à¹ˆ Vector DB à¸à¸£à¹‰à¸­à¸¡ metadata
        
        Args:
            filepath: Path to source file
            chunks: List of text chunks
            metadata: Document-level metadata (from metadata_extractor)
        """
        # à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¹ˆà¸²à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰à¸­à¸­à¸à¸à¹ˆà¸­à¸™ (à¸–à¹‰à¸²à¸¡à¸µ)
        self.collection.delete(where={"source": filepath})
        
        if not chunks:
            logger.warning(f"âš ï¸ No chunks to index for {filepath}")
            return

        # Prepare metadata for each chunk
        base_metadata = metadata or {}
        ids = [f"{filepath}_{i}" for i in range(len(chunks))]
        metadatas = []
        
        for i in range(len(chunks)):
            chunk_metadata = {
                "source": filepath,
                "filename": base_metadata.get('filename', os.path.basename(filepath)),
                "chunk_index": i,
                "doc_type": base_metadata.get('doc_type', 'general'),
                "language": base_metadata.get('language', 'th'),
                "has_dates": base_metadata.get('has_dates', False),
                "last_updated": base_metadata.get('last_updated', datetime.datetime.now().isoformat())
            }
            
            # Add academic_years and semesters as JSON strings (ChromaDB limitation)
            if 'academic_years' in base_metadata and base_metadata['academic_years']:
                chunk_metadata['academic_years'] = ','.join(base_metadata['academic_years'])
            
            if 'semesters' in base_metadata and base_metadata['semesters']:
                chunk_metadata['semesters'] = ','.join(map(str, base_metadata['semesters']))
            
            metadatas.append(chunk_metadata)
        
        # à¸ªà¸£à¹‰à¸²à¸‡ Embeddings
        embeddings = self.model.encode(
            [f"passage: {c}" for c in chunks],
            normalize_embeddings=True
        ).tolist()

        # Add to ChromaDB
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=chunks,
            metadatas=metadatas
        )
        
        logger.info(f"âœ… Indexed {len(chunks)} chunks from {os.path.basename(filepath)}")

    def search(self, query: str, k: int = 5, filter_dict: Optional[Dict] = None) -> List[Dict]:
        """
        à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹ƒà¸à¸¥à¹‰à¹€à¸„à¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸ªà¸¸à¸” à¸à¸£à¹‰à¸­à¸¡à¸à¸£à¸­à¸‡ metadata
        
        Args:
            query: Search query
            k: Number of results (à¸ˆà¸°à¸”à¸¶à¸‡à¸¡à¸²à¸à¸‚à¸¶à¹‰à¸™à¹€à¸à¸·à¹ˆà¸­ post-filter)
            filter_dict: Metadata filters (e.g., {'doc_type': 'calendar', 'academic_year': '2568'})
        
        Returns:
            List of dicts with chunk, source, score, metadata
        """
        # Create query embedding
        query_embedding = self.model.encode(
            f"query: {query}",
            normalize_embeddings=True
        ).tolist()

        # âœ… ChromaDB à¸£à¸­à¸‡à¸£à¸±à¸š filter à¹€à¸”à¸µà¸¢à¸§à¸•à¹ˆà¸­ query
        # Strategy: à¹ƒà¸Šà¹‰ filter à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸à¹ˆà¸­à¸™ à¹à¸¥à¹‰à¸§ post-filter à¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­
        where_clause = None
        post_filters = {}
        
        if filter_dict:
            # Priority 1: doc_type (à¸–à¹‰à¸²à¸¡à¸µ)
            if 'doc_type' in filter_dict:
                where_clause = {'doc_type': filter_dict['doc_type']}
                logger.debug(f"ğŸ” Pre-filter: doc_type={filter_dict['doc_type']}")
            
            # Priority 2: language (à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ doc_type)
            elif 'language' in filter_dict:
                where_clause = {'language': filter_dict['language']}
                logger.debug(f"ğŸ” Pre-filter: language={filter_dict['language']}")
            
            # à¹€à¸à¹‡à¸š filters à¸­à¸·à¹ˆà¸™à¹„à¸§à¹‰à¸ªà¸³à¸«à¸£à¸±à¸š post-processing
            if 'academic_year' in filter_dict:
                post_filters['academic_year'] = filter_dict['academic_year']
            
            if 'semester' in filter_dict:
                post_filters['semester'] = str(filter_dict['semester'])

        # âœ… Query ChromaDB with increased k for post-filtering
        query_k = k * 3 if post_filters else k
        
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=query_k,
                where=where_clause
            )
        except Exception as e:
            logger.error(f"âŒ ChromaDB query error: {e}")
            # Fallback: query without filters
            try:
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=query_k
                )
            except Exception as e2:
                logger.error(f"âŒ Fallback query failed: {e2}")
                return []

        # Format results
        formatted_results = []
        if results['documents'] and results['documents'][0]:
            for i in range(len(results['documents'][0])):
                metadata = results['metadatas'][0][i]
                
                # âœ… Post-filter: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š academic_year à¹à¸¥à¸° semester
                if post_filters:
                    # Filter by academic_year
                    if 'academic_year' in post_filters:
                        academic_years = metadata.get('academic_years', '')
                        if post_filters['academic_year'] not in academic_years:
                            continue
                    
                    # Filter by semester
                    if 'semester' in post_filters:
                        semesters = metadata.get('semesters', '')
                        if post_filters['semester'] not in semesters:
                            continue
                
                formatted_results.append({
                    "chunk": results['documents'][0][i],
                    "source": metadata.get('source', ''),
                    "score": 1.0 - results['distances'][0][i],  # Convert distance to similarity
                    "metadata": metadata
                })
        
        # âœ… Return top k after post-filtering
        filtered_count = len(formatted_results)
        formatted_results = formatted_results[:k]
        
        if post_filters:
            logger.info(f"âœ… Post-filtered: {filtered_count} â†’ {len(formatted_results)} results")
        
        return formatted_results
    
    def get_all_chunks(self) -> List[Dict]:
        """
        Get all chunks from database (for BM25 indexing)
        
        Returns:
            List of dicts with chunk, source, index
        """
        try:
            # Get all documents (ChromaDB limit is 100,000)
            results = self.collection.get()
            
            chunks = []
            if results['documents']:
                for i, doc in enumerate(results['documents']):
                    chunks.append({
                        'chunk': doc,
                        'source': results['metadatas'][i].get('source', ''),
                        'index': results['metadatas'][i].get('chunk_index', i)
                    })
            
            logger.info(f"ğŸ“š Retrieved {len(chunks)} chunks for indexing")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ Error getting all chunks: {e}")
            return []

# Create a singleton instance
vector_manager = VectorManager()
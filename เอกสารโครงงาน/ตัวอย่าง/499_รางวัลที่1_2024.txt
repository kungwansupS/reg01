===== PAGE 1 =====
After semantic chunking and BERT embedding generation, we need to visualizedocument structure. Using graph-based visualization techniques, the system plotsconcept relationships and claim-evidence chains, enabling clear representation ofsemantic connections between document sections as shown in the figure below.
      DISTILL is primarily designed to process and analyze research papers in PDFformat, focusing on extracting structured information from complex academicdocuments. DISTILL implements a three-stage preprocessing pipeline to transformraw documents into structured, analyzable content:Text Cleaning transforms raw documents by normalizing formats and removingconversion artifacts, ensuring clean and consistent text output.1.
Semantic Chunking divides documents into coherent sections whilemaintaining contextual relationships between segments.2.
Extraction Phase identifies and organizes key textual elements, preparingstructured content for knowledge graph construction.3.
  Throughout this pipeline, DISTILL prioritizes document structure preservation,ensuring the integrity of section relationships and contextual information.
D A T A 	 - 	 P R E P R O C E S S I N G 
E x p l o r a t o r y 	 D a t a 	 A n a l y s i s 
   The evaluation framework integrates standard metrics (ROUGE, BERTScore),drawing from Zhang et al.'s (2020). This approach enables comprehensiveevaluation of both summary quality and semantic retention.
E v a l u a t i o n 
R e s u l t s 	 & 	 D i s c u s s i o n 
   The evaluation was conducted using 100 samples from the CNN/DailyMail dataset. The resultsdemonstrate DISTILL's superior performance across multiple metrics. While both approaches showcompetitive ROUGE scores, DISTILL achieves notably higher results with ROUGE-1 (0.51 vs 0.42),ROUGE-2 (0.40 vs 0.39), and ROUGE-L (0.45 vs 0.41). Although the baseline model shows slightlyhigher cosine similarity (0.80 vs 0.74), DISTILL's graph-based approach generates more semanticmatch summaries compared to direct summarization model.
Semantic Text Processing
     For semantic text processing, DISTILL adopts the BERT embeddingapproach similar to Devlin et al. (2019), utilizing its contextual embeddingsfor semantic chunking. However, unlike traditional BERT applications, DISTILLimplements a modified semantic similarity threshold mechanism (0.95) tomaintain coherent document segments while preserving cross-sectionalrelationships.
Ref: Devlin et al., 2019
M E T H O D O L O G Y 
Graph Construction
     The graph construction phase draws inspiration from TextRank(Mihalcea and Tarau, 2004), but extends beyond basic keyword extraction.DISTILL incorporates a hierarchical graph structure similar to Liu & Lapata's(2019) approach, while introducing a concept weighting system. Eachconcept's importance is calculated using a weighted formula shown below.
     The system also employs SpaCy's NLP library for entity recognition,which enables accurate identification of concepts and their syntactic rolesfor weight calculation
  For summarization, DISTILL uses the Falconsai/text_summarizationmodel as a baseline traditional summarizer. This pretrained model providesa benchmark for comparing the effectiveness of DISTILL's graph-basedapproach.
Summarization
      This research presents DISTILL, a framework combining semantic analysis with graph-based knowledge representation to enhance document summarization.Current summarization approaches often lose critical relationships between concepts and fail to maintain the hierarchical structure of academic documents,potentially misrepresenting key findings and their supporting evidence. DISTILL addresses these challenges through a multi-stage pipeline of semantic chunking,knowledge graph construction, and summary generation, preserving both semantic relationships and document structure in the final output.
DISTILL: Detailed Intelligent Summarization for TextInformation Learning and Linkage
A u t h o r 	 -  Suphakit Ng
A d v i s o r 	 - 	 Associate Professor Dr.Jakramate Bootkrajang 
A B S T R A C T 
       Document summarization has become increasingly critical in managing the growing volume of academic literature, yet current approaches often fail to capturethe intricate web of relationships between concepts, claims, and evidence that form the backbone of academic discourse. DISTILL addresses these limitations througha graph-based approach that weights and preserves semantic relationships. By integrating BERT embeddings with graph-based knowledge representation, DISTILLcreates a semantic framework that understands both local relationships between concepts and global document structure, enabling summaries that maintain theintellectual integrity of academic works while preserving their argumentative structure.
I N T R O D U C T I O N